{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataset = loadmat('hw04_data.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Represents a layer in the neural network. Takes in input dimension, output dimension \n",
    "# and activation function of the layer, which is set to Identity as default.\n",
    "class Layer(torch.nn.Module):\n",
    "\n",
    "    # Constructor for the Layer class.\n",
    "    def __init__(self, inputDim, outputDim, activation = \"Identity\"):\n",
    "        \n",
    "        super(Layer, self).__init__()\n",
    "        self.inputDim = inputDim\n",
    "        self.outputDim = outputDim\n",
    "\n",
    "        # Depending on the string input representing which activation function to use\n",
    "        # correctly assigns the activation.\n",
    "        if (activation == \"ReLU\"):\n",
    "            self.activation = torch.nn.ReLU()\n",
    "        elif (activation == \"Tanh\"):\n",
    "            self.activation = torch.nn.Tanh()\n",
    "        elif (activation == \"Identity\"):\n",
    "            self.activation = torch.nn.Identity()\n",
    "        elif (activation == \"Sigmoid\"):\n",
    "            self.activation = torch.nn.Sigmoid()\n",
    "\n",
    "        # How to find the total weighted input\n",
    "        self.l1 = torch.nn.Linear(self.inputDim, self.outputDim)\n",
    "\n",
    "        \n",
    "    # Signature: input vector X -> activation/output\n",
    "    # Combines the weights and activations of the previous layer, and applies an activation function\n",
    "    def forward(self, x):\n",
    "        entry = self.l1(x)\n",
    "        output = self.activation(entry)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents a complete neural network with multiple layers. Takes in a list of layers.\n",
    "class NN:\n",
    "\n",
    "    # Constructor for the NN class\n",
    "    def __init__(self, layers):\n",
    "        \n",
    "        super(NN, self).__init__()        \n",
    "        self.layers = layers\n",
    "        param_list = []\n",
    "        for layer in layers:\n",
    "            param_list += list(layer.parameters())\n",
    "        self.parameters = torch.nn.ParameterList(param_list)\n",
    "\n",
    "    # Signature: Input data X -> Prediction from NN\n",
    "    # Passes the data through the neural network and returns a prediction\n",
    "    def forward(self, x): \n",
    "        entry = x\n",
    "        for layer in self.layers:\n",
    "            entry = layer.forward(entry)\n",
    "        return entry\n",
    "             \n",
    "    \n",
    "    # Signature:\n",
    "    # \n",
    "    def score(self, x_test, y_test):\n",
    "        x_test_tensor = torch.FloatTensor(X_test.values)\n",
    "        test_results = model.forward(x_test_tensor)\n",
    "        results = [torch.argmax(res).item() for res in test_results]\n",
    "        \n",
    "        total = 0\n",
    "        for i in range(len(results)):\n",
    "            if results[i] == y_test[i]:\n",
    "                total += 1\n",
    "\n",
    "        return total/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature: number of layers, layer sizes for each layer, string representing activation function -> Neural Network\n",
    "# constructs the neural network\n",
    "def nn(n, sl, activation):\n",
    "    layers = [Layer(sl[i], sl[i+1], activation) for i in range(0, len(sl) - 1)]\n",
    "    return NN(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature: NN, nxm dataset, nxp dataset, rxs dataset, rxt dataset -> modifies the model\n",
    "# training the neural network\n",
    "def train_nn(model, x_train, y_train, x_test, y_test):\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = torch.optim.Adam(model.parameters, lr=learning_rate)\n",
    "\n",
    "    for t in range(5000):\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        x_train_tensor = torch.FloatTensor(x_train)\n",
    "        y_pred = model.forward(x_train_tensor)\n",
    "\n",
    "        # Compute and print loss.\n",
    "        y_train_tensor = y_train\n",
    "        loss = criterion(y_pred, y_train_tensor)\n",
    "        # if t % 100 == 99:\n",
    "        #     print(t, loss.item())\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs and runs the neural network with n layers and given sizes of each layer with activation function on the given data\n",
    "def feed_forward_nn(n, sl, data, activation):\n",
    "\n",
    "    # split the data into training and testing\n",
    "    X_trn = data[\"X_trn\"]\n",
    "    y_trn = data[\"y_trn\"]\n",
    "    X_tst = data[\"X_tst\"]\n",
    "    y_tst = data[\"y_tst\"]\n",
    "    X_trn_torch = torch.FloatTensor(X_trn)\n",
    "\n",
    "    # create the model based on the desired number of layers and nodes at each layer\n",
    "    sl.insert(0, X_trn.shape[1])\n",
    "    sl.append(2)\n",
    "    model = nn(n, sl, activation)\n",
    "\n",
    "    # adjust the training y dataset to change -1 to 0 to better handle the data and make it a tensor\n",
    "    y_updated = []\n",
    "    for item in y_trn:\n",
    "        if item[0] == 1:\n",
    "            y_updated.append([1])\n",
    "        else:\n",
    "            y_updated.append([0])\n",
    "    y_trn_torch = torch.LongTensor(np.array(y_updated)).reshape(-1)\n",
    "\n",
    "    #y_trn_torch = torch.LongTensor(y_trn).reshape(-1)\n",
    "    X_tst_torch = torch.FloatTensor(X_tst)\n",
    "    y_tst_torch = torch.LongTensor(y_tst).reshape(-1)\n",
    "\n",
    "    # train the model\n",
    "    train_nn(model, X_trn_torch, y_trn_torch, X_tst_torch, y_tst_torch)\n",
    "\n",
    "    # print out the parameters\n",
    "    print(\"Final neural network layers:\\n\")\n",
    "    for parameter in model.parameters:\n",
    "        print(parameter)\n",
    "\n",
    "    # print out the activation of the final layer\n",
    "    results = model.forward(X_tst_torch)\n",
    "    tst_results = []\n",
    "    for row in results:\n",
    "        if row[0] > row[1]:\n",
    "            tst_results.append([-1])\n",
    "        else:\n",
    "            tst_results.append([1])\n",
    "\n",
    "    tst_results = np.array(tst_results) \n",
    "    print(\"\\nPrediction from testing set:\\n\")\n",
    "    print(tst_results)\n",
    "\n",
    "    print(\"\\nAccuracy:\")\n",
    "    print(np.sum(tst_results == y_tst) / len(y_tst))\n",
    "\n",
    "    return tst_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Final neural network layers:\n\nParameter containing:\ntensor([[ 0.6943,  0.6859],\n        [ 0.3550, -0.8490]], requires_grad=True)\nParameter containing:\ntensor([-0.0144,  0.5951], requires_grad=True)\nParameter containing:\ntensor([[ 0.8563, -0.4860],\n        [ 0.0702,  0.4182],\n        [-0.3972, -0.4066],\n        [ 0.8415, -0.1295],\n        [-0.6129,  0.3872],\n        [ 0.7006, -0.4364],\n        [ 0.8387,  0.2803],\n        [-0.6094, -0.6769],\n        [ 0.5045, -0.1216],\n        [ 0.7174, -0.3226],\n        [ 0.9137, -0.2464],\n        [-0.1859, -0.1894],\n        [-0.3565, -0.3899],\n        [ 0.0915, -0.8014],\n        [-0.8030,  0.5757],\n        [ 0.6872,  0.1325],\n        [-0.7818,  0.8162],\n        [-0.4297, -0.4235],\n        [ 0.5602,  0.2475],\n        [ 0.4564,  0.3905],\n        [-0.5725, -0.5824],\n        [-0.1456,  0.8445],\n        [ 0.5297,  0.5293],\n        [-0.1233,  0.4787],\n        [ 0.5169, -0.6596],\n        [-0.6245, -0.2269],\n        [ 0.2338, -0.6243],\n        [ 0.8019, -0.6258],\n        [ 0.0159,  0.5770],\n        [-0.2829, -0.1451],\n        [-0.4252, -0.0120],\n        [ 0.9923, -0.3748],\n        [ 0.2996,  0.7168],\n        [ 0.4698, -0.0271],\n        [-0.6219, -0.6731],\n        [-0.3670, -0.2186],\n        [ 0.5304,  0.6388],\n        [-0.9668,  0.4262],\n        [ 0.8697, -0.7507],\n        [-0.9287,  0.6109],\n        [ 0.1395, -0.0099],\n        [ 0.3855, -0.3111],\n        [-0.0045,  0.4549],\n        [ 0.9722, -0.6330],\n        [ 0.5121,  0.3822],\n        [ 0.3458,  0.4045],\n        [-0.2029,  0.5898],\n        [ 0.1153,  0.3355],\n        [-0.4389,  0.1923],\n        [-0.2738,  0.3535]], requires_grad=True)\nParameter containing:\ntensor([ 0.3462,  0.0200, -0.4033, -0.4702,  0.3728,  0.2949,  0.8901, -0.4812,\n        -0.1700,  0.5595, -0.5023,  0.7785, -0.1075,  0.7769,  0.0337, -0.5666,\n         0.0189, -0.1387, -0.5311,  0.9177, -0.0559, -0.1779,  0.0151,  0.1083,\n         0.4766, -0.6889,  0.4386,  0.4616, -0.1556, -0.5907, -0.5692, -0.3974,\n         0.0346,  0.8670, -0.6763, -0.6224,  0.8195,  0.1720,  0.5527, -0.0239,\n         0.7517,  0.4752, -0.5311,  0.6376,  0.7465, -0.5391,  0.6199,  0.0975,\n        -0.6141,  0.0335], requires_grad=True)\nParameter containing:\ntensor([[ 0.1260, -0.2030,  0.0668,  ..., -0.1690,  0.0388, -0.3079],\n        [-0.1132,  0.0237,  0.0649,  ...,  0.0428,  0.1089,  0.0860],\n        [-0.0519,  0.0547,  0.1177,  ..., -0.0315, -0.1393,  0.0928],\n        ...,\n        [ 0.0585, -0.1081, -0.1080,  ...,  0.0907,  0.0800,  0.0318],\n        [ 0.1701, -0.2279,  0.0083,  ..., -0.0892, -0.1255, -0.2493],\n        [-0.0467, -0.0929,  0.0450,  ...,  0.0341, -0.0340, -0.0878]],\n       requires_grad=True)\nParameter containing:\ntensor([ 8.9804e-02, -4.1983e-02,  2.5291e-02, -7.7422e-02,  2.3958e-01,\n        -7.3759e-02,  4.0892e-02,  1.3472e-01, -1.5672e-02,  2.1001e-01,\n        -7.8293e-02,  1.6636e-01,  1.1151e-01,  9.6921e-02,  2.8382e-01,\n         2.4985e-01,  1.6724e-01,  2.2553e-01,  1.8450e-04,  1.3622e-01,\n         1.1020e-02,  1.1965e-01, -9.6992e-02,  1.7108e-01, -3.2669e-02],\n       requires_grad=True)\nParameter containing:\ntensor([[-0.3654,  0.3807, -0.1005, -0.1705, -0.1127,  0.1732,  0.2110,  0.0454,\n         -0.0209, -0.1683,  0.2755, -0.3242,  0.1197,  0.3029,  0.2058,  0.3280,\n          0.2327,  0.2091,  0.2123,  0.0283,  0.0860,  0.0895, -0.1817, -0.1410,\n          0.1162],\n        [ 0.1920, -0.4899,  0.1786,  0.1746,  0.4512, -0.4814, -0.2687, -0.1983,\n         -0.1746,  0.3999, -0.4164,  0.1459, -0.1696, -0.3809,  0.3003,  0.3919,\n         -0.4419,  0.4164, -0.4675,  0.0423,  0.1689, -0.4519,  0.1216,  0.4234,\n         -0.0688]], requires_grad=True)\nParameter containing:\ntensor([0.0737, 0.2279], requires_grad=True)\n\nPrediction from testing set:\n\n[[ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [-1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]\n [-1]\n [ 1]\n [-1]\n [-1]\n [ 1]\n [-1]\n [ 1]\n [ 1]\n [ 1]]\n\nAccuracy:\n1.0\n"
    }
   ],
   "source": [
    "# Example: apply the feed forward neural net on the given dataset\n",
    "feed_forward = feed_forward_nn(3, [2, 50, 25], dataset, \"ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}